name: Run MLE-Bench Agent

on:
  workflow_dispatch:
    inputs:
      competition_set:
        description: 'Competition set to run'
        required: true
        type: choice
        options:
          - custom-set.txt
          - low.txt (Lite - 21 competitions)
          - medium.txt (38 competitions)
          - high.txt (14 competitions)
          - dev.txt (6 competitions)
        default: 'custom-set.txt'

      custom_competitions:
        description: 'Custom competitions (one per line, overrides competition_set if provided)'
        required: false
        type: string

      dry_run:
        description: 'Dry run mode (validate without executing)'
        required: false
        type: boolean
        default: false

      target_gpu:
        description: 'Target specific GPU machine (leave empty for any available)'
        required: false
        type: choice
        options:
          - any
          - gpu-1
          - gpu-2
        default: 'any'

jobs:
  run-agent:
    # Support concurrency: 2 (no group = both can run simultaneously)
    runs-on:
      - self-hosted
      - gpu
      - ${{ github.event.inputs.target_gpu != 'any' && github.event.inputs.target_gpu || 'gpu' }}

    timeout-minutes: 1440  # 24 hours max

    steps:
      - name: Show run configuration
        run: |
          echo "=========================================="
          echo "MLE-Bench Run Configuration"
          echo "=========================================="
          echo "Branch: ${{ github.ref_name }}"
          echo "Competition set: ${{ github.event.inputs.competition_set }}"
          echo "Dry run: ${{ github.event.inputs.dry_run }}"
          echo "Target GPU: ${{ github.event.inputs.target_gpu }}"
          echo "Runner: $(hostname)"
          echo "Run ID: ${{ github.run_id }}"
          echo "Run Number: ${{ github.run_number }}"
          echo "=========================================="

      - name: Clean workspace
        run: |
          echo "Cleaning workspace..."
          cd $GITHUB_WORKSPACE || true
          git clean -fdx || true
          git reset --hard HEAD || true

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          clean: true
          lfs: true

      - name: Setup competition set
        run: |
          cd mle-bench/experiments/splits

          # If custom competitions provided, use those
          if [ -n "${{ github.event.inputs.custom_competitions }}" ]; then
            echo "Using custom competitions list:"
            echo "${{ github.event.inputs.custom_competitions }}" | tee custom-set.txt
          else
            # Extract filename from selection (e.g., "low.txt (Lite - 21 competitions)" -> "low.txt")
            COMP_FILE=$(echo "${{ github.event.inputs.competition_set }}" | awk '{print $1}')
            echo "Using competition set: $COMP_FILE"

            # If not custom-set.txt, copy to custom-set.txt for script
            if [ "$COMP_FILE" != "custom-set.txt" ]; then
              cp "$COMP_FILE" custom-set.txt
              echo "Copied $COMP_FILE to custom-set.txt"
            fi
          fi

          echo ""
          echo "Competitions to run:"
          cat custom-set.txt

      - name: Setup Python environment
        run: |
          cd mle-bench

          # Create venv if it doesn't exist
          if [ ! -d "venv" ]; then
            echo "Creating virtual environment..."
            python3 -m venv venv
          fi

          # Activate venv and install mlebench
          source venv/bin/activate
          echo "Installing mlebench..."
          pip install -e . --quiet

          echo "✅ Python environment ready"
          echo "   Python: $(which python)"
          echo "   mlebench: $(python -c 'import mlebench; print(mlebench.__file__)')"

      - name: Run MLE-Bench
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          IMAGE_TAG: agent_v5_kaggle:run-${{ github.run_id }}
          DRY_RUN: ${{ github.event.inputs.dry_run }}
        run: |
          cd mle-bench

          # Activate venv
          source venv/bin/activate

          echo "Starting MLE-Bench with:"
          echo "  Working directory: $(pwd)"
          echo "  IMAGE_TAG: $IMAGE_TAG"
          echo "  DRY_RUN: $DRY_RUN"
          echo "  Python: $(which python)"
          echo ""

          # Run the script
          ./RUN_AGENT_V5_KAGGLE.sh

      - name: Upload results
        if: always() && github.event.inputs.dry_run == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: mle-bench-results-${{ github.ref_name }}-run${{ github.run_number }}
          path: ${{ github.workspace }}/mle-bench/runs/
          retention-days: 30

      - name: Upload grading report
        if: always() && github.event.inputs.dry_run == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: grading-report-${{ github.ref_name }}-run${{ github.run_number }}
          path: |
            ${{ github.workspace }}/mle-bench/runs/*/*_grading_report.json
            ${{ github.workspace }}/mle-bench/runs/*/results.json
          retention-days: 90

      - name: Display results summary
        if: always() && github.event.inputs.dry_run == 'false'
        run: |
          cd ${{ github.workspace }}/mle-bench

          # Find latest run
          RUN_GROUP=$(ls -t runs/ 2>/dev/null | head -1)

          if [ -n "$RUN_GROUP" ]; then
            echo ""
            echo "=========================================="
            echo "Results Summary"
            echo "=========================================="
            echo "Run group: $RUN_GROUP"
            echo ""

            # Find grading report (either format)
            GRADING_REPORT=$(find "runs/$RUN_GROUP/" -name "*_grading_report.json" -o -name "results.json" | head -1)

            if [ -n "$GRADING_REPORT" ]; then
              echo "Grading Results:"
              cat "$GRADING_REPORT" | head -50
            else
              echo "⚠️  Grading results not found"
              echo "Files in run directory:"
              ls -la "runs/$RUN_GROUP/"
            fi

            echo ""
            echo "Download artifacts above to see full results"
          else
            echo "⚠️  No results found"
          fi

      - name: Cleanup Docker
        if: always()
        run: |
          echo "Cleaning up Docker resources..."

          # Kill any running containers from this run
          echo "Stopping containers for this run..."
          RUNNING_CONTAINERS=$(docker ps -q --filter "ancestor=agent_v5_kaggle:run-${{ github.run_id }}")
          if [ -n "$RUNNING_CONTAINERS" ]; then
            echo "Found running containers: $RUNNING_CONTAINERS"
            docker stop $RUNNING_CONTAINERS || true
            docker rm $RUNNING_CONTAINERS || true
          else
            echo "No running containers found for this run"
          fi

          # Remove the specific image we built
          docker image rm agent_v5_kaggle:run-${{ github.run_id }} 2>/dev/null || true

          # Clean up stopped containers
          docker container prune -f

          # Clean up dangling images
          docker image prune -f

          # Show remaining disk usage
          echo ""
          echo "Docker disk usage after cleanup:"
          docker system df

      - name: Cleanup workspace
        if: always()
        run: |
          # Clean up large files but keep code for debugging
          cd ${{ github.workspace }}/mle-bench

          # Remove downloaded competition data (can be re-downloaded)
          rm -rf data/ 2>/dev/null || true

          # Remove large model checkpoints if any
          find runs/ -name "*.pth" -o -name "*.pt" -o -name "*.h5" | xargs rm -f 2>/dev/null || true

          echo "Workspace cleanup complete"
